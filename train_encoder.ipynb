{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset 2775\n",
      "torch.Size([16, 1, 40, 168])\n",
      "torch.Size([16, 40])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loading checkpoint from ./checkpoints\\checkpoint_epoch_621.pth...\n",
      "Resuming training from epoch 621...\n",
      "training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavan\\AppData\\Local\\Temp\\ipykernel_23792\\2323315938.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(latest_checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [622/2], Loss: 0.0847\n",
      "torch.Size([16, 4, 10])\n",
      "Original : tensor([[6, 2, 9, 6],\n",
      "        [1, 4, 7, 3],\n",
      "        [9, 0, 9, 3]])\n",
      "outut  torch.Size([4, 10])\n",
      "Predictions:  tensor([[6, 2, 9, 6],\n",
      "        [1, 9, 7, 3],\n",
      "        [9, 0, 9, 3]])\n",
      "Epoch [623/2], Loss: 0.0589\n",
      "torch.Size([16, 4, 10])\n",
      "Original : tensor([[6, 2, 9, 6],\n",
      "        [1, 4, 7, 3],\n",
      "        [9, 0, 9, 3]])\n",
      "outut  torch.Size([4, 10])\n",
      "Predictions:  tensor([[6, 6, 9, 6],\n",
      "        [1, 4, 7, 3],\n",
      "        [9, 0, 9, 3]])\n",
      "hi\n",
      "Latest checkpoint saved to ./checkpoints\\latest_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom Dataset Class\n",
    "class MNISTDigitsDataset(Dataset):\n",
    "    def __init__(self, root_dirs, transform=None):\n",
    "        #self.image_dir = image_dir\n",
    "        #self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "        #self.transform = transform\n",
    "\n",
    "        self.image_paths = []\n",
    "        for root_dir in root_dirs:\n",
    "            for file in os.listdir(root_dir):\n",
    "                if file.endswith('.png'):\n",
    "                    self.image_paths.append(os.path.join(root_dir, file))\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        #image_path = os.path.join(self.image_dir, image_name)\n",
    "        \n",
    "\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "        image_name = os.path.basename(image_path)\n",
    "        # Extract label from the filename (e.g., '0428.png' -> [0, 4, 2, 8])\n",
    "        label = [int(digit) for digit in image_name.split('.')[0]]\n",
    "        \n",
    "        # Convert label to one-hot encoding (4 digits, 10 classes per digit)\n",
    "        one_hot_label = torch.zeros(40, dtype=torch.long)  # 10 classes * 4 digits = 40\n",
    "        for i, digit in enumerate(label):\n",
    "            one_hot_label[i * 10 + digit] = 1  # Set the corresponding class to 1\n",
    "        \n",
    "        # Apply transformations (if any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, one_hot_label\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "#    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),  # Random rotation ±15° and shifts up to 10%\n",
    "    transforms.Resize((40, 168)),  # Resize image to the correct size\n",
    "    transforms.ToTensor(),         # Convert image to Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize (for grayscale images)\n",
    "])\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "#image_dir = r\"./exterim/images\"  # Update with your path\n",
    "root_dirs = [\n",
    "    r\"./exterim/images\",  # Update with your paths\n",
    "    r\"./exterim/images2\"  # Add additional directories here\n",
    "]\n",
    "\n",
    "dataset = MNISTDigitsDataset(root_dirs=root_dirs, transform=transform)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "print(\"length of dataset\",len(dataset))\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "# DataLoader for batching\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoader for training and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Example usage\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)  # Should print torch.Size([32, 1, 40, 168])\n",
    "    print(labels.shape)  # Should print torch.Size([32, 40]) for one-hot encoded labels\n",
    "\n",
    "    print(labels[0])\n",
    "    # Display the first image in the batch\n",
    "    # plt.imshow(images[0].squeeze(0), cmap='gray')  # Remove the channel dimension for display\n",
    "    # plt.show()\n",
    "\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNISTDigitModel(nn.Module):\n",
    "    def __init__(self, num_blocks, kernel_size, activation, pool, dropout):\n",
    "        super(MNISTDigitModel, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.pool = pool\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = 1  # Grayscale input images\n",
    "        out_channels = 64  # Initial number of filters\n",
    "        \n",
    "        # Add convolutional blocks\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding='same'),\n",
    "                self._get_activation(activation),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding='same'),\n",
    "                self._get_activation(activation),\n",
    "                self._get_pool(pool),\n",
    "                nn.Dropout(dropout)\n",
    "            ))\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2  # Double the filters after each block\n",
    "        \n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(*layers)\n",
    "        \n",
    "        # Dummy input to calculate the flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 40, 168)\n",
    "            flattened_size = self.conv_blocks(dummy_input).numel()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, 512),\n",
    "            self._get_activation(activation),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 40)  # 40 output classes (10 per digit for 4 digits)\n",
    "        )\n",
    "    \n",
    "    def _get_activation(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Activation not supported\")\n",
    "    \n",
    "    def _get_pool(self, pool):\n",
    "        if pool == 'max':\n",
    "            return nn.MaxPool2d(2)\n",
    "        elif pool == 'avg':\n",
    "            return nn.AvgPool2d(2)\n",
    "        else:\n",
    "            raise ValueError(\"Pooling method not supported\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = x.view(-1, 4, 10)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "# model = MNISTDigitModel(num_blocks=1, kernel_size=3, activation='relu', pool='max', dropout=0.1)\n",
    "# print(model)\n",
    "\n",
    "\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "new_dropout = 0.1\n",
    "model = MNISTDigitModel(num_blocks=5, kernel_size=3, activation='relu', pool='max', dropout=new_dropout)\n",
    "\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "latest_checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_621.pth')\n",
    "start_epoch = 0  # Default start epoch\n",
    "if os.path.exists(latest_checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {latest_checkpoint_path}...\")\n",
    "    checkpoint = torch.load(latest_checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "\n",
    "\n",
    "new_learning_rate = 0.00001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=new_learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"training started\")\n",
    "# Training Loop\n",
    "num_epochs = 2  # Set the number of epochs\n",
    "for epoch in range(start_epoch,start_epoch+num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:       \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Reshape labels to [batch_size, 4, 10] (assuming they are one-hot encoded)\n",
    "        labels = labels.view(-1, 4, 10)  # Shape: [batch_size, 4, 10]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)  # Shape: [batch_size, 4, 10]\n",
    "\n",
    "        # Compute loss for each digit independently\n",
    "        loss = 0\n",
    "        for i in range(4):  # Loop through each digit\n",
    "            digit_labels = labels[:, i, :].argmax(dim=1)  # Convert one-hot to class index\n",
    "            digit_outputs = outputs[:, i, :]\n",
    "\n",
    "            # print(\"-- \",digit_labels)\n",
    "\n",
    "            # CrossEntropyLoss expects [batch_size, num_classes] for inputs and [batch_size] for labels\n",
    "            loss += criterion(digit_outputs, digit_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print loss for every epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,  # Save the next epoch number\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint\n",
    "    if epoch%10==0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "\n",
    "        \n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for images, labels in test_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(images)  # Shape: [batch_size, 4, 10]\n",
    "            # print(images.shape)\n",
    "            # print(outputs.shape)\n",
    "            \n",
    "            labels = labels.view(-1, 4, 10)\n",
    "            print(labels.shape) \n",
    "            original_label = torch.argmax(labels, dim=2) \n",
    "            print(\"Original :\", original_label[:3])\n",
    "\n",
    "            print(\"outut \",outputs[0].shape)\n",
    "            predictions = torch.argmax(outputs, dim=2)  # Shape: [4] with the predicted digit for each group\n",
    "            print(\"Predictions: \",predictions[:3])\n",
    "\n",
    "            break\n",
    "    \n",
    "print(\"hi\")\n",
    "\n",
    "# Update the latest checkpoint pointer\n",
    "latest_checkpoint_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')\n",
    "torch.save(checkpoint, latest_checkpoint_path)\n",
    "print(f\"Latest checkpoint saved to {latest_checkpoint_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
