{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset 2775\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom Dataset Class\n",
    "class MNISTDigitsDataset(Dataset):\n",
    "    def __init__(self, root_dirs, transform=None):\n",
    "        #self.image_dir = image_dir\n",
    "        #self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "        #self.transform = transform\n",
    "\n",
    "        self.image_paths = []\n",
    "        for root_dir in root_dirs:\n",
    "            for file in os.listdir(root_dir):\n",
    "                if file.endswith('.png'):\n",
    "                    self.image_paths.append(os.path.join(root_dir, file))\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        #image_path = os.path.join(self.image_dir, image_name)\n",
    "        \n",
    "\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "        image_name = os.path.basename(image_path)\n",
    "        # Extract label from the filename (e.g., '0428.png' -> [0, 4, 2, 8])\n",
    "        label = [int(digit) for digit in image_name.split('.')[0]]\n",
    "        \n",
    "\n",
    "        #added\n",
    "        label_num = torch.sum(torch.tensor(label)).item()  # Sum the elements and convert to a Python scalar\n",
    "\n",
    "\n",
    "        # Convert label to one-hot encoding (4 digits, 10 classes per digit)\n",
    "        one_hot_label = torch.zeros(40, dtype=torch.long)  # 10 classes * 4 digits = 40\n",
    "        for i, digit in enumerate(label):\n",
    "            one_hot_label[i * 10 + digit] = 1  # Set the corresponding class to 1\n",
    "        \n",
    "        # Apply transformations (if any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, one_hot_label ,label_num ,image_name\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "#    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),  # Random rotation ±15° and shifts up to 10%\n",
    "    transforms.Resize((40, 168)),  # Resize image to the correct size\n",
    "    transforms.ToTensor(),         # Convert image to Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize (for grayscale images)\n",
    "])\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "#image_dir = r\"./exterim/images\"  # Update with your path\n",
    "root_dirs = [\n",
    "    r\"./exterim/images\",  # Update with your paths\n",
    "    r\"./exterim/images2\"  # Add additional directories here\n",
    "]\n",
    "\n",
    "dataset = MNISTDigitsDataset(root_dirs=root_dirs, transform=transform)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "print(\"length of dataset\",len(dataset))\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "# DataLoader for batching\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoader for training and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#class 1\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNISTDigitModel(nn.Module):\n",
    "    def __init__(self, num_blocks, kernel_size, activation, pool, dropout):\n",
    "        super(MNISTDigitModel, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.pool = pool\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = 1  # Grayscale input images\n",
    "        out_channels = 64  # Initial number of filters\n",
    "        \n",
    "        # Add convolutional blocks\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding='same'),\n",
    "                self._get_activation(activation),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding='same'),\n",
    "                self._get_activation(activation),\n",
    "                self._get_pool(pool),\n",
    "                nn.Dropout(dropout)\n",
    "            ))\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2  # Double the filters after each block\n",
    "            \n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(*layers)\n",
    "        \n",
    "        # Dummy input to calculate the flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 40, 168)\n",
    "            flattened_size = self.conv_blocks(dummy_input).numel()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, 512),\n",
    "            self._get_activation(activation),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 40)  # 40 output classes (10 per digit for 4 digits)\n",
    "        )\n",
    "    \n",
    "    def _get_activation(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Activation not supported\")\n",
    "    \n",
    "    def _get_pool(self, pool):\n",
    "        if pool == 'max':\n",
    "            return nn.MaxPool2d(2)\n",
    "        elif pool == 'avg':\n",
    "            return nn.AvgPool2d(2)\n",
    "        else:\n",
    "            raise ValueError(\"Pooling method not supported\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = x.view(-1, 4, 10)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNISTSumModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTSumModel, self).__init__()\n",
    "        \n",
    "        # MLP layers (flatten the 1x4x10 output)\n",
    "        self.fc1 = nn.Linear(40, 64)  # 40 input features (4 digits * 10 classes)\n",
    "        self.fc22 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)  # Output a single scalar (the sum of digits)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply softmax to each 10-length vector (per digit)\n",
    "        x = x.float()\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        \n",
    "        # Flatten the input (4 digits * 10 classes)\n",
    "        x = x.view(-1, 40)  # Shape becomes (batch_size, 40)\n",
    "\n",
    "        # MLP layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc22(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Single scalar output\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Instantiate the models\n",
    "new_dropout = 0.1\n",
    "digit_model = MNISTDigitModel(num_blocks=5, kernel_size=3, activation='relu', pool='max', dropout=new_dropout)\n",
    "sum_model = MNISTSumModel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# checkpoint_dir = './checkpoints'\n",
    "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "# latest_checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_621.pth')\n",
    "# start_epoch = 0\n",
    "# ## not needed as we have combined now\n",
    "# if os.path.exists(latest_checkpoint_path):\n",
    "#     print(f\"Loading checkpoint from {latest_checkpoint_path}...\")\n",
    "#     checkpoint = torch.load(latest_checkpoint_path)\n",
    "#     digit_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     start_epoch = checkpoint['epoch']\n",
    "#     print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "\n",
    "#     for param in digit_model.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# ## not needed as we have combined now\n",
    "# sum_model_path = \"checkpoints/decoder.pth\"\n",
    "# if os.path.exists(sum_model_path):\n",
    "#     print(f\"Loading pretrained MNISTSumModel from {sum_model_path}...\")\n",
    "#     sum_model.load_state_dict(torch.load(sum_model_path))\n",
    "\n",
    "#     # for param in sum_model.parameters():\n",
    "#     #     param.requires_grad = False\n",
    "\n",
    "\n",
    "for param in digit_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading combined model checkpoint from ./checkpoints_comb/checkpoint_epoch_700.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavan\\AppData\\Local\\Temp\\ipykernel_13432\\2977658306.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 700...\n",
      "Epoch [701/10000], Loss: 0.2615\n",
      "Sample : 4418.png\n",
      "Original Sum: 17\n",
      "Predicted Sum: 17.041715621948242\n",
      "Sample : 2308.png\n",
      "Original Sum: 13\n",
      "Predicted Sum: 12.943526268005371\n",
      "Sample : 5567.png\n",
      "Original Sum: 23\n",
      "Predicted Sum: 23.0585880279541\n",
      "Epoch [702/10000], Loss: 0.2345\n",
      "Sample : 4418.png\n",
      "Original Sum: 17\n",
      "Predicted Sum: 17.047210693359375\n",
      "Sample : 2308.png\n",
      "Original Sum: 13\n",
      "Predicted Sum: 12.908239364624023\n",
      "Sample : 5567.png\n",
      "Original Sum: 23\n",
      "Predicted Sum: 22.90321922302246\n",
      "Epoch [703/10000], Loss: 0.3244\n",
      "Sample : 4418.png\n",
      "Original Sum: 17\n",
      "Predicted Sum: 17.04605484008789\n",
      "Sample : 2308.png\n",
      "Original Sum: 13\n",
      "Predicted Sum: 12.990662574768066\n",
      "Sample : 5567.png\n",
      "Original Sum: 23\n",
      "Predicted Sum: 23.034543991088867\n",
      "Epoch [704/10000], Loss: 0.1865\n",
      "Sample : 4418.png\n",
      "Original Sum: 17\n",
      "Predicted Sum: 17.21051788330078\n",
      "Sample : 2308.png\n",
      "Original Sum: 13\n",
      "Predicted Sum: 13.010443687438965\n",
      "Sample : 5567.png\n",
      "Original Sum: 23\n",
      "Predicted Sum: 23.127132415771484\n",
      "Epoch [705/10000], Loss: 0.3130\n",
      "Sample : 4418.png\n",
      "Original Sum: 17\n",
      "Predicted Sum: 16.95234489440918\n",
      "Sample : 2308.png\n",
      "Original Sum: 13\n",
      "Predicted Sum: 12.97558879852295\n",
      "Sample : 5567.png\n",
      "Original Sum: 23\n",
      "Predicted Sum: 23.012901306152344\n",
      "Epoch [706/10000], Loss: 0.3067\n",
      "Sample : 4418.png\n",
      "Original Sum: 17\n",
      "Predicted Sum: 17.105924606323242\n",
      "Sample : 2308.png\n",
      "Original Sum: 13\n",
      "Predicted Sum: 13.057286262512207\n",
      "Sample : 5567.png\n",
      "Original Sum: 23\n",
      "Predicted Sum: 22.986146926879883\n",
      "Epoch [707/10000], Loss: 0.2254\n",
      "Sample : 4418.png\n",
      "Original Sum: 17\n",
      "Predicted Sum: 17.141313552856445\n",
      "Sample : 2308.png\n",
      "Original Sum: 13\n",
      "Predicted Sum: 12.935967445373535\n",
      "Sample : 5567.png\n",
      "Original Sum: 23\n",
      "Predicted Sum: 22.950244903564453\n",
      "Epoch [708/10000], Loss: 0.3112\n",
      "Sample : 4418.png\n",
      "Original Sum: 17\n",
      "Predicted Sum: 17.117361068725586\n",
      "Sample : 2308.png\n",
      "Original Sum: 13\n",
      "Predicted Sum: 13.091445922851562\n",
      "Sample : 5567.png\n",
      "Original Sum: 23\n",
      "Predicted Sum: 23.107051849365234\n",
      "Epoch [709/10000], Loss: 0.2218\n",
      "Sample : 4418.png\n",
      "Original Sum: 17\n",
      "Predicted Sum: 17.049537658691406\n",
      "Sample : 2308.png\n",
      "Original Sum: 13\n",
      "Predicted Sum: 12.890884399414062\n",
      "Sample : 5567.png\n",
      "Original Sum: 23\n",
      "Predicted Sum: 22.93206214904785\n",
      "Epoch [710/10000], Loss: 0.2571\n",
      "Checkpoint saved at ./checkpoints_comb\\checkpoint_epoch_710.pth\n",
      "Sample : 4418.png\n",
      "Original Sum: 17\n",
      "Predicted Sum: 17.11489486694336\n",
      "Sample : 2308.png\n",
      "Original Sum: 13\n",
      "Predicted Sum: 13.04064655303955\n",
      "Sample : 5567.png\n",
      "Original Sum: 23\n",
      "Predicted Sum: 23.067293167114258\n",
      "Epoch [711/10000], Loss: 0.2382\n",
      "Sample : 4418.png\n",
      "Original Sum: 17\n",
      "Predicted Sum: 17.0900821685791\n",
      "Sample : 2308.png\n",
      "Original Sum: 13\n",
      "Predicted Sum: 13.106471061706543\n",
      "Sample : 5567.png\n",
      "Original Sum: 23\n",
      "Predicted Sum: 22.96696662902832\n",
      "Epoch [712/10000], Loss: 0.2026\n",
      "Sample : 4418.png\n",
      "Original Sum: 17\n",
      "Predicted Sum: 16.970661163330078\n",
      "Sample : 2308.png\n",
      "Original Sum: 13\n",
      "Predicted Sum: 12.95163345336914\n",
      "Sample : 5567.png\n",
      "Original Sum: 23\n",
      "Predicted Sum: 22.776269912719727\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 61\u001b[0m\n\u001b[0;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m _, sum_output \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# sum_output = sum_output.view(-1) \u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Compute loss (compare output to label_num)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion_sum(sum_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), labels_sum\u001b[38;5;241m.\u001b[39mfloat())  \u001b[38;5;66;03m# Squeeze to remove extra dimensions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     digit_output_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: [batch_size, 4, 10]\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     sum_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_model(digit_output_encoded)  \u001b[38;5;66;03m# Shape: [batch_size, 1]\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m digit_output_encoded, sum_output\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 68\u001b[0m, in \u001b[0;36mMNISTDigitModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 68\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[0;32m     71\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pavan\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Define combined model\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, digit_model, sum_model):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.digit_model = digit_model\n",
    "        self.sum_model = sum_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        digit_output_encoded = self.digit_model(x)  # Shape: [batch_size, 4, 10]\n",
    "        sum_output = self.sum_model(digit_output_encoded)  # Shape: [batch_size, 1]\n",
    "        return digit_output_encoded, sum_output\n",
    "\n",
    "combined_model = CombinedModel(digit_model, sum_model)\n",
    "\n",
    "\n",
    "\n",
    "# Optimizer and criterion\n",
    "new_learning_rate = 0.0001 ##0.0001 for CNN\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, combined_model.parameters()), lr=new_learning_rate)\n",
    "# optimizer = optim.Adam([\n",
    "#     {'params': filter(lambda p: p.requires_grad, digit_model.parameters()), 'lr': 0.00001},\n",
    "# ])\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "#     {'params': digit_model.parameters(), 'lr': 0.00001},  # Learning rate for digit_model\n",
    "#     {'params': sum_model.parameters(), 'lr': 0.0},  # Frozen sum_model (effectively ignored)\n",
    "# ])\n",
    "\n",
    "# Load the combined model checkpoint\n",
    "checkpoint_path = './checkpoints_comb/checkpoint_epoch_700.pth'\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading combined model checkpoint from {checkpoint_path}...\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Load the combined model weights\n",
    "    combined_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # Set the starting epoch for resuming training\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "\n",
    "# criterion_digit = nn.CrossEntropyLoss()  # For digit classification\n",
    "criterion_sum = nn.MSELoss()  # For sum regression (used only for evaluation)\n",
    "\n",
    "\n",
    "# Training loop for combined model\n",
    "num_epochs = 10000\n",
    "checkpoint_dir = './checkpoints_comb'  # Directory for saving checkpoints\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)  # Ensure the checkpoint directory exists\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    combined_model.train()  # Set model to training mode\n",
    "    # running_loss_digit = 0.0\n",
    "    # running_loss_sum = 0.0\n",
    "    running_loss = 0.0\n",
    "    for images, _, labels_sum, _ in train_loader:  # Adjust based on your dataset\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass\n",
    "        \n",
    "        _, sum_output = combined_model(images)\n",
    "        # sum_output = sum_output.view(-1) \n",
    "        # Compute loss (compare output to label_num)\n",
    "        loss = criterion_sum(sum_output.view(-1), labels_sum.float())  # Squeeze to remove extra dimensions\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': combined_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    # Evaluation on test_loader\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for images, _, labels_sum, nam in test_loader:  # Adjust based on your dataset\n",
    "            # Forward pass\n",
    "            _, sum_output = combined_model(images)\n",
    "\n",
    "\n",
    "            # Optionally, compare predicted and actual values\n",
    "            for i in range(3):\n",
    "                print(f\"Sample :\",nam[i])\n",
    "                print(f\"Original Sum: {labels_sum[i]}\")\n",
    "                print(f\"Predicted Sum: {sum_output[i].item()}\")  # Use .item() to get the scalar value\n",
    "\n",
    "            break  # Just evaluate the first batch and exit the loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample : 4418.png\n",
      "Original Sum: 17\n",
      "Predicted Sum: 16.9477481842041\n",
      "Sample : 2308.png\n",
      "Original Sum: 13\n",
      "Predicted Sum: 12.901043891906738\n",
      "Sample : 5567.png\n",
      "Original Sum: 23\n",
      "Predicted Sum: 22.8297061920166\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test_loader\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for images, _, labels_sum, nam in test_loader:  # Adjust based on your dataset\n",
    "        # Forward pass\n",
    "        _, sum_output = combined_model(images)\n",
    "\n",
    "\n",
    "        # Optionally, compare predicted and actual values\n",
    "        for i in range(3):\n",
    "            print(f\"Sample :\",nam[i])\n",
    "            print(f\"Original Sum: {labels_sum[i]}\")\n",
    "            print(f\"Predicted Sum: {sum_output[i].item()}\")  # Use .item() to get the scalar value\n",
    "\n",
    "        break  # Just evaluate the first batch and exit the loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Even at this stage,its able to predict correct sum, after round off will give correct answer\n",
    "#### Training it more will reduce MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
