{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset 2775\n",
      "torch.Size([16, 1, 40, 168])\n",
      "torch.Size([16, 40])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor(21)\n",
      "5772.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom Dataset Class\n",
    "class MNISTDigitsDataset(Dataset):\n",
    "    def __init__(self, root_dirs, transform=None):\n",
    "        #self.image_dir = image_dir\n",
    "        #self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "        #self.transform = transform\n",
    "\n",
    "        self.image_paths = []\n",
    "        for root_dir in root_dirs:\n",
    "            for file in os.listdir(root_dir):\n",
    "                if file.endswith('.png'):\n",
    "                    self.image_paths.append(os.path.join(root_dir, file))\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        #image_path = os.path.join(self.image_dir, image_name)\n",
    "        \n",
    "\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "        image_name = os.path.basename(image_path)\n",
    "        # Extract label from the filename (e.g., '0428.png' -> [0, 4, 2, 8])\n",
    "        label = [int(digit) for digit in image_name.split('.')[0]]\n",
    "        \n",
    "\n",
    "        #added\n",
    "        label_num = torch.sum(torch.tensor(label)).item()  # Sum the elements and convert to a Python scalar\n",
    "\n",
    "\n",
    "        # Convert label to one-hot encoding (4 digits, 10 classes per digit)\n",
    "        one_hot_label = torch.zeros(40, dtype=torch.long)  # 10 classes * 4 digits = 40\n",
    "        for i, digit in enumerate(label):\n",
    "            one_hot_label[i * 10 + digit] = 1  # Set the corresponding class to 1\n",
    "        \n",
    "        # Apply transformations (if any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, one_hot_label ,label_num ,image_name\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "#    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),  # Random rotation ±15° and shifts up to 10%\n",
    "    transforms.Resize((40, 168)),  # Resize image to the correct size\n",
    "    transforms.ToTensor(),         # Convert image to Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize (for grayscale images)\n",
    "])\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "#image_dir = r\"./exterim/images\"  # Update with your path\n",
    "root_dirs = [\n",
    "    r\"./exterim/images\",  # Update with your paths\n",
    "    r\"./exterim/images2\"  # Add additional directories here\n",
    "]\n",
    "\n",
    "dataset = MNISTDigitsDataset(root_dirs=root_dirs, transform=transform)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "print(\"length of dataset\",len(dataset))\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "# DataLoader for batching\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoader for training and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Example usage\n",
    "for images, labels ,label_num,image_name in train_loader:\n",
    "    print(images.shape)  # Should print torch.Size([32, 1, 40, 168])\n",
    "    print(labels.shape)  # Should print torch.Size([32, 40]) for one-hot encoded labels\n",
    "\n",
    "    print(labels[0])\n",
    "    print(label_num[0])\n",
    "    print(image_name[0])\n",
    "    # Display the first image in the batch\n",
    "    # plt.imshow(images[0].squeeze(0), cmap='gray')  # Remove the channel dimension for display\n",
    "    # plt.show()\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class 1\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNISTDigitModel(nn.Module):\n",
    "    def __init__(self, num_blocks, kernel_size, activation, pool, dropout):\n",
    "        super(MNISTDigitModel, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.pool = pool\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = 1  # Grayscale input images\n",
    "        out_channels = 64  # Initial number of filters\n",
    "        \n",
    "        # Add convolutional blocks\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding='same'),\n",
    "                self._get_activation(activation),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding='same'),\n",
    "                self._get_activation(activation),\n",
    "                self._get_pool(pool),\n",
    "                nn.Dropout(dropout)\n",
    "            ))\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2  # Double the filters after each block\n",
    "            \n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(*layers)\n",
    "        \n",
    "        # Dummy input to calculate the flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 40, 168)\n",
    "            flattened_size = self.conv_blocks(dummy_input).numel()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, 512),\n",
    "            self._get_activation(activation),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 40)  # 40 output classes (10 per digit for 4 digits)\n",
    "        )\n",
    "    \n",
    "    def _get_activation(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Activation not supported\")\n",
    "    \n",
    "    def _get_pool(self, pool):\n",
    "        if pool == 'max':\n",
    "            return nn.MaxPool2d(2)\n",
    "        elif pool == 'avg':\n",
    "            return nn.AvgPool2d(2)\n",
    "        else:\n",
    "            raise ValueError(\"Pooling method not supported\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = x.view(-1, 4, 10)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNISTSumModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTSumModel, self).__init__()\n",
    "        \n",
    "        # MLP layers (flatten the 1x4x10 output)\n",
    "        self.fc1 = nn.Linear(40, 64)  # 40 input features (4 digits * 10 classes)\n",
    "        self.fc22 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)  # Output a single scalar (the sum of digits)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply softmax to each 10-length vector (per digit)\n",
    "        x = x.float()\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        \n",
    "        # Flatten the input (4 digits * 10 classes)\n",
    "        x = x.view(-1, 40)  # Shape becomes (batch_size, 40)\n",
    "\n",
    "        # MLP layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc22(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Single scalar output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./checkpoints\\checkpoint_epoch_621.pth...\n",
      "Resuming training from epoch 621...\n",
      "Loading pretrained MNISTSumModel from checkpoints/decoder.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavan\\AppData\\Local\\Temp\\ipykernel_10780\\468967892.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(latest_checkpoint_path)\n",
      "C:\\Users\\Pavan\\AppData\\Local\\Temp\\ipykernel_10780\\468967892.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sum_model.load_state_dict(torch.load(sum_model_path))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Instantiate the models\n",
    "new_dropout = 0.1\n",
    "digit_model = MNISTDigitModel(num_blocks=5, kernel_size=3, activation='relu', pool='max', dropout=new_dropout)\n",
    "sum_model = MNISTSumModel()\n",
    "\n",
    "\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "latest_checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_621.pth')\n",
    "start_epoch = 0\n",
    "if os.path.exists(latest_checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {latest_checkpoint_path}...\")\n",
    "    checkpoint = torch.load(latest_checkpoint_path)\n",
    "    digit_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "\n",
    "    # for param in digit_model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "\n",
    "\n",
    "sum_model_path = \"checkpoints/decoder.pth\"\n",
    "if os.path.exists(sum_model_path):\n",
    "    print(f\"Loading pretrained MNISTSumModel from {sum_model_path}...\")\n",
    "    sum_model.load_state_dict(torch.load(sum_model_path))\n",
    "\n",
    "    # for param in sum_model.parameters():\n",
    "    #     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 4, 10])\n",
      "\n",
      "Image: 6296.png\n",
      "Predicted sum: 23.00\n",
      "Actual sum: 23.00\n",
      "\n",
      "Image: 2513.png\n",
      "Predicted sum: 11.01\n",
      "Actual sum: 11.00\n",
      "\n",
      "Image: 1215.png\n",
      "Predicted sum: 9.00\n",
      "Actual sum: 9.00\n",
      "\n",
      "Image: 6008.png\n",
      "Predicted sum: 14.00\n",
      "Actual sum: 14.00\n",
      "\n",
      "Image: 5990.png\n",
      "Predicted sum: 23.00\n",
      "Actual sum: 23.00\n",
      "\n",
      "Image: 8838.png\n",
      "Predicted sum: 27.00\n",
      "Actual sum: 27.00\n",
      "\n",
      "Image: 5907.png\n",
      "Predicted sum: 21.01\n",
      "Actual sum: 21.00\n",
      "\n",
      "Image: 3802.png\n",
      "Predicted sum: 13.00\n",
      "Actual sum: 13.00\n",
      "\n",
      "Image: 3644.png\n",
      "Predicted sum: 17.00\n",
      "Actual sum: 17.00\n",
      "\n",
      "Image: 9810.png\n",
      "Predicted sum: 18.00\n",
      "Actual sum: 18.00\n",
      "\n",
      "Image: 9416.png\n",
      "Predicted sum: 20.00\n",
      "Actual sum: 20.00\n",
      "\n",
      "Image: 6429.png\n",
      "Predicted sum: 21.00\n",
      "Actual sum: 21.00\n",
      "\n",
      "Image: 2578.png\n",
      "Predicted sum: 22.01\n",
      "Actual sum: 22.00\n",
      "\n",
      "Image: 6980.png\n",
      "Predicted sum: 23.00\n",
      "Actual sum: 23.00\n",
      "\n",
      "Image: 3140.png\n",
      "Predicted sum: 8.00\n",
      "Actual sum: 8.00\n",
      "\n",
      "Image: 7862.png\n",
      "Predicted sum: 23.01\n",
      "Actual sum: 23.00\n"
     ]
    }
   ],
   "source": [
    "# Set models to evaluation mode\n",
    "digit_model.eval()\n",
    "sum_model.eval()\n",
    "\n",
    "# Lists to store results\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "# Disable gradient calculations for inference\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, _, labels_sum, image_names) in enumerate(test_loader):\n",
    "        # Step 1: Get output from digit model\n",
    "        # Assuming digit_output has shape [batch_size, 4, 10]\n",
    "        digit_output = digit_model(images)  # Shape: [batch_size, 4, 10]\n",
    "        digit_output = digit_output.unsqueeze(1)  # Shape: [batch_size, 1, 4, 10]\n",
    "        # print(digit_output.shape)\n",
    "\n",
    "        # Prepare a list to hold the one-hot labels for each batch element\n",
    "        one_hot_labels = []\n",
    "        # Iterate through all batch elements\n",
    "        for batch_idx in range(digit_output.shape[0]):\n",
    "            predictions = torch.argmax(digit_output[batch_idx], dim=2)\n",
    "            one_hot_label = torch.zeros(40, dtype=torch.long)\n",
    "            for i, digit in enumerate(predictions[0]):\n",
    "                one_hot_label[i * 10 + digit] = 1\n",
    "            one_hot_label = one_hot_label.view(-1, 4, 10)\n",
    "            one_hot_labels.append(one_hot_label)\n",
    "\n",
    "        # Stack the one-hot labels into a batch of size [batch_size, 1, 4, 10]\n",
    "        one_hot_labels = torch.stack(one_hot_labels, dim=0)\n",
    "        print(one_hot_labels.shape)\n",
    "\n",
    "\n",
    "        # Step 2: Feed the one-hot labels to the sum model\n",
    "        sum_output = sum_model(one_hot_labels) \n",
    "\n",
    "        # Get predicted digit probabilities\n",
    "        digit_probs = F.softmax(digit_output, dim=-1)  # Shape: [batch_size, 4, 10]\n",
    "        \n",
    "        # Get predicted digits (indices of maximum probability for each position)\n",
    "        predicted_digits = torch.argmax(digit_probs, dim=-1)  # Shape: [batch_size, 4]\n",
    "        \n",
    "        # Get predicted sums\n",
    "        predicted_sums = sum_output.squeeze()\n",
    "        actual_sums = labels_sum.float()\n",
    "        \n",
    "        # Store predictions and actuals\n",
    "        # For example, to append the sum of predictions for the batch\n",
    "        all_predictions.append(predicted_sums.sum().item())  # Sum and convert to scalar\n",
    "        all_actuals.append(actual_sums.sum().item())        # Sum and convert to scalar\n",
    "\n",
    "        \n",
    "        # Print results for this batch\n",
    "        for i in range(len(images)):\n",
    "            print(f\"\\nImage: {image_names[i]}\")\n",
    "            print(f\"Predicted sum: {predicted_sums[i]:.2f}\")\n",
    "            print(f\"Actual sum: {actual_sums[i]:.2f}\")\n",
    "            \n",
    "        # Optional: break after first batch to see sample results\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
